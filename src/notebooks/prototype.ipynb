{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'venv (Python 3.12.5)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '\"/Users/matyno/Projects/AI projects/HAL9001/venv/bin/python\" -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from typing import Dict, Any, List\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
    "from langchain.tools import BaseTool\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "from langgraph.graph import Graph\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize LLM\n",
    "anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "if not anthropic_api_key:\n",
    "    raise ValueError(\"ANTHROPIC_API_KEY environment variable is not set\")\n",
    "\n",
    "llm = ChatAnthropic(model=\"claude-3-opus-20240229\",\n",
    "                    anthropic_api_key=anthropic_api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define ScheduleTool\n",
    "class ScheduleInput(BaseModel):\n",
    "    action: str = Field(...,\n",
    "                        description=\"The action to perform on the schedule (add, remove, get)\")\n",
    "    time: str = Field(..., description=\"The time for the task in HH:MM format\")\n",
    "    task: str = Field(..., description=\"The task description\")\n",
    "\n",
    "\n",
    "class ScheduleTool(BaseTool):\n",
    "    name = \"schedule_tool\"\n",
    "    description = \"Use this tool to manage the daily schedule. Actions: add, remove, get\"\n",
    "    args_schema = ScheduleInput\n",
    "    schedule: Dict[str, str] = {}\n",
    "\n",
    "    def _run(self, action: str, time: str, task: str) -> Dict[str, Any]:\n",
    "        if action == \"add\":\n",
    "            self.schedule[time] = task\n",
    "            return {\"status\": \"success\", \"message\": f\"Added task '{task}' at {time}\"}\n",
    "        elif action == \"remove\":\n",
    "            if time in self.schedule:\n",
    "                del self.schedule[time]\n",
    "                return {\"status\": \"success\", \"message\": f\"Removed task at {time}\"}\n",
    "            else:\n",
    "                return {\"status\": \"error\", \"message\": f\"No task found at {time}\"}\n",
    "        elif action == \"get\":\n",
    "            return {\"status\": \"success\", \"schedule\": self.schedule}\n",
    "        else:\n",
    "            return {\"status\": \"error\", \"message\": f\"Invalid action: {action}\"}\n",
    "\n",
    "    async def _arun(self, action: str, time: str, task: str) -> Dict[str, Any]:\n",
    "        return self._run(action, time, task)\n",
    "\n",
    "\n",
    "# Initialize ScheduleTool and ToolNode\n",
    "schedule_tool = ScheduleTool()\n",
    "tool_node = ToolNode(tools=[schedule_tool])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define graph nodes\n",
    "\n",
    "\n",
    "def generate_response(state):\n",
    "    messages = state[\"messages\"]\n",
    "    tool_results = state.get(\"tool_results\", [])\n",
    "\n",
    "    if tool_results:\n",
    "        messages.append(AIMessage(\n",
    "            content=f\"I've updated the schedule. Here's what I did:\\n{tool_results[-1]}\"))\n",
    "\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"messages\": messages + [response], \"response\": response.content}\n",
    "\n",
    "def call_tool(state):\n",
    "    messages = state[\"messages\"]\n",
    "    response = state[\"response\"]\n",
    "\n",
    "    tool_input = {\n",
    "        \"name\": \"schedule_tool\",\n",
    "        \"arguments\": {\n",
    "            \"action\": \"get\",\n",
    "            \"time\": \"00:00\",\n",
    "            \"task\": \"\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return {\"tool_input\": tool_input}\n",
    "\n",
    "\n",
    "def should_call_tool(state):\n",
    "    response = state[\"response\"]\n",
    "    if \"schedule\" in response.lower():\n",
    "        return \"call_tool\"\n",
    "    else:\n",
    "        return \"end\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Node `call_tool` is not reachable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m workflow\u001b[38;5;241m.\u001b[39madd_edge(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcall_tool\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m workflow\u001b[38;5;241m.\u001b[39madd_edge(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerate_response\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m graph \u001b[38;5;241m=\u001b[39m \u001b[43mworkflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Function to run the chat\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchat\u001b[39m(user_input: \u001b[38;5;28mstr\u001b[39m, messages: List):\n",
      "File \u001b[0;32m~/Projects/AI projects/HAL9001/venv/lib/python3.12/site-packages/langgraph/graph/graph.py:384\u001b[0m, in \u001b[0;36mGraph.compile\u001b[0;34m(self, checkpointer, interrupt_before, interrupt_after, debug)\u001b[0m\n\u001b[1;32m    381\u001b[0m interrupt_after \u001b[38;5;241m=\u001b[39m interrupt_after \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[1;32m    383\u001b[0m \u001b[38;5;66;03m# validate the graph\u001b[39;00m\n\u001b[0;32m--> 384\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterrupt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m*\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minterrupt_after\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m*\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    388\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;66;03m# create empty compiled graph\u001b[39;00m\n\u001b[1;32m    393\u001b[0m compiled \u001b[38;5;241m=\u001b[39m CompiledGraph(\n\u001b[1;32m    394\u001b[0m     builder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    395\u001b[0m     nodes\u001b[38;5;241m=\u001b[39m{},\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    405\u001b[0m     debug\u001b[38;5;241m=\u001b[39mdebug,\n\u001b[1;32m    406\u001b[0m )\n",
      "File \u001b[0;32m~/Projects/AI projects/HAL9001/venv/lib/python3.12/site-packages/langgraph/graph/graph.py:360\u001b[0m, in \u001b[0;36mGraph.validate\u001b[0;34m(self, interrupt)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodes:\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m node \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m all_targets:\n\u001b[0;32m--> 360\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNode `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` is not reachable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m target \u001b[38;5;129;01min\u001b[39;00m all_targets:\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m target \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodes \u001b[38;5;129;01mand\u001b[39;00m target \u001b[38;5;241m!=\u001b[39m END:\n",
      "\u001b[0;31mValueError\u001b[0m: Node `call_tool` is not reachable"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create the graph\n",
    "workflow = Graph()\n",
    "\n",
    "workflow.add_node(\"generate_response\", generate_response)\n",
    "workflow.add_node(\"call_tool\", call_tool)\n",
    "workflow.add_node(\"tool\", tool_node)\n",
    "\n",
    "workflow.set_entry_point(\"generate_response\")\n",
    "workflow.add_edge(\"generate_response\", should_call_tool)\n",
    "workflow.add_edge(\"call_tool\", \"tool\")\n",
    "workflow.add_edge(\"tool\", \"generate_response\")\n",
    "\n",
    "graph = workflow.compile()\n",
    "\n",
    "# Function to run the chat\n",
    "\n",
    "\n",
    "def chat(user_input: str, messages: List):\n",
    "    messages.append(HumanMessage(content=user_input))\n",
    "    result = graph.invoke({\"messages\": messages})\n",
    "    return result[\"messages\"], result[\"response\"]\n",
    "\n",
    "\n",
    "# Initialize the chat\n",
    "system_message = SystemMessage(content=\"\"\"You are an AI assistant named HAL-9001, designed to help users manage their daily schedule. Your personality is slightly sarcastic and witty, but always helpful. You can add tasks to the schedule, remove them, and retrieve the current schedule. Use the schedule_tool when needed to manage the schedule. Always confirm changes with the user before making them.\"\"\")\n",
    "\n",
    "messages = [system_message]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create widgets for the chat interface\n",
    "output = widgets.Output()\n",
    "text_input = widgets.Text(\n",
    "    description=\"You:\", placeholder=\"Type your message here...\")\n",
    "send_button = widgets.Button(description=\"Send\")\n",
    "\n",
    "\n",
    "def on_send_button_clicked(b):\n",
    "    user_input = text_input.value\n",
    "    text_input.value = \"\"\n",
    "\n",
    "    with output:\n",
    "        print(f\"You: {user_input}\")\n",
    "        if user_input.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
    "            print(\"HAL-9001: Farewell, human. Try not to miss me too much.\")\n",
    "            print(\"\\nYour final schedule:\")\n",
    "            final_schedule = schedule_tool._run(\"get\", \"\", \"\")[\"schedule\"]\n",
    "            for time, task in sorted(final_schedule.items()):\n",
    "                print(f\"{time}: {task}\")\n",
    "        else:\n",
    "            messages, response = chat(user_input, messages)\n",
    "            print(f\"HAL-9001: {response}\")\n",
    "\n",
    "\n",
    "send_button.on_click(on_send_button_clicked)\n",
    "\n",
    "# Display the chat interface\n",
    "display(widgets.VBox([output, widgets.HBox([text_input, send_button])]))\n",
    "\n",
    "# Initial greeting\n",
    "with output:\n",
    "    print(\"HAL-9001: Greetings, human. I'm HAL-9001, your slightly sarcastic but incredibly efficient scheduling assistant. How may I optimize your day?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
